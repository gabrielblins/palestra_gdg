{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielblins/palestra_gdg/blob/main/notebooks/GoogleExtended_OpenAILangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integração OpenAI - Langchain\n",
        "\n",
        "Vamos ao que interessa."
      ],
      "metadata": {
        "id": "vhfhLRR_gdJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Software base\n",
        "Vamos instalar o langchain e algumas outras coisas"
      ],
      "metadata": {
        "id": "sDQljnkZhQun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xPT4kaRggZPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17d4bcb-7ded-4503-c8d5-309ab03b2906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m137.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m138.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q langchain unstructured pinecone-client openai tiktoken python-dotenv faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVFq1LiSZh02",
        "outputId": "fe232673-309c-4e01-939c-545c6a3465af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integração com Pinecone\n",
        "\n",
        "Para realizar uma integração completa usando exclusivamente o Maritaca, a API precisaria de um endpoint para captura dos embeddings, mas isso ainda não está disponível, então, a seguir iremos utilizar a OpenAI para criar as representações dos textos, enviar para o Pinecone e usar o Maritaca no final do fluxo. Funciona assim:\n",
        "\n",
        "1. Capturamos os textos que queremos trabalhar, neste caso, a Portaria de Consolidação N 1 do Ministério da Saúde.\n",
        "\n",
        "2. Vamos quebrar o texto em pequenas partes.\n",
        "\n",
        "3. Interagir com a OpenAI e capturar os embeddings dos trechos criados.\n",
        "\n",
        "4. Enviar esses trechos para o Pinecone.\n",
        "\n",
        "Perceba que os passos acima são executados uma só vez ou sempre que houver necessidade de atualizar a estrutura. O resultado é uma base com diversos vetores que representam cada um dos trechos que criamos. O Pinecone, ou qualquer banco do mesmo tipo, viabiliza que sejam executadas consultas por similaridade. Dessa forma:\n",
        "\n",
        "1. Agora que temos a base podemos enviar uma pergunta ou um texto qualquer, mas temos que criar a representação da pergunta e neste momento iremos chamar novamente a OpenAI para captura dos embeddings.\n",
        "\n",
        "2. Em seguida, a Langchain interage com o Pinecone e pede para retornar os N vetores que mais se parecem com a pergunta/texto que enviamos.\n",
        "\n",
        "Deste ponto em diante, já temos acesso aos N trechos semelhantes e, portanto, são os que possívelmente respondem a nossa pergunta. Vamos usar eles como contexto do prompt final com Maritaca."
      ],
      "metadata": {
        "id": "2CKaxw0KzoiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import OnlinePDFLoader\n",
        "\n",
        "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2304.07880.pdf\")"
      ],
      "metadata": {
        "id": "dvQPhEoWznwY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "spnh8rpw_SYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed068e3-9d38-4ad2-ce33-b85e93f4105a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'Você tem {len(data)} documento(s) na base')\n",
        "print (f'Há {len(data[0].page_content)} caracteres no documento')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wBNh_VG_OD7",
        "outputId": "e3534653-f12b-437d-93ff-7c7b9781d365"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Você tem 1 documento(s) na base\n",
            "Há 65146 caracteres no documento\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100, separators=['\\n', ' ', ''])\n",
        "texts = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "ceoaIfvi_2hi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'Agora há {len(texts)} documentos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FYQmtVtAAVM",
        "outputId": "8f37688e-0d55-4ab9-d507-6157c306bbbe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agora há 192 documentos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[50].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qLVU_xlWAGMx",
        "outputId": "f822ff1f-c564-4631-b1c5-2ba543452669"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Through our Portuguese pretraining, we observed that the improvement in NPM was higher in native datasets than that in translated datasets. For Sabi´a-65B, improvements over LLaMA-65B were mostly from the native subset. We hypothesize that this is due to the “mechanistic” nature of translated datasets: since they were translated from English, the baseline model already possesses the knowledge needed to solve them and gains little from learning the linguistic, syntactic, and grammatical knowledge of the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuração da OpenAI e Pinecone\n",
        "\n",
        "Recomendo ler como o Pinecone funciona, há muita documentação pela internet.\n",
        "\n",
        "O código a seguir realiza a integração entre Pinecone e OpenAI e para isso você vai precisar de chaves. Para manter as que uso de forma privada, criei um arquivo .env com o seguinte formato:\n",
        "\n",
        "```\n",
        "    OPENAI_API_KEY=key\n",
        "    PINECONE_API_KEY=key\n",
        "    PINECONE_API_ENV=key\n",
        "    PINECONE_INDEX=key\n",
        "```\n",
        "\n",
        "Chamei de .env e coloquei na raiz do colab. Você pode fazer o mesmo para testar este notebook."
      ],
      "metadata": {
        "id": "5RvC7gq7CHgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import pinecone\n",
        "import os"
      ],
      "metadata": {
        "id": "yBo08jDEAcj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfbe5bf-6c00-4e5c-ecc9-0a43bc27e54b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
        "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
        "PINECONE_API_ENV = os.environ['PINECONE_API_ENV']\n",
        "INDEX_NAME = os.environ['PINECONE_INDEX']"
      ],
      "metadata": {
        "id": "kPmRWM9WAvwb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "LTqfAXzrdnr7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pinecone\n",
        "#pinecone.init(\n",
        "#    api_key=PINECONE_API_KEY,\n",
        "#    environment=PINECONE_API_ENV\n",
        "#)\n",
        "#index_name = INDEX_NAME"
      ],
      "metadata": {
        "id": "EW6Q-oY6Dks0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use o código a seguir para criar a base\n",
        "#docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name, namespace=index_name)\n",
        "\n",
        "# Use o código a seguir se já tiver um índice criado\n",
        "# docsearch = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)"
      ],
      "metadata": {
        "id": "pgHNDerwEeW2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuração da FAISS vector store"
      ],
      "metadata": {
        "id": "UI8NwLl-jD74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "import pickle\n",
        "\n",
        "def store_embeddings(docs, embeddings, sotre_name, path):\n",
        "\n",
        "    vectorStore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(vectorStore, f)\n",
        "\n",
        "def load_embeddings(sotre_name, path):\n",
        "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"rb\") as f:\n",
        "        VectorStore = pickle.load(f)\n",
        "    return VectorStore\n"
      ],
      "metadata": {
        "id": "HfDb_80Aap43"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = FAISS.from_texts([t.page_content for t in texts], embeddings)"
      ],
      "metadata": {
        "id": "xUGe9aLMcHYM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base construída\n",
        "\n",
        "Agora temos nossa base vetorizada! Nosso PDF contendo os dados da Portaria de Consolidação já está no Pinecone, vamos testar!\n",
        "\n",
        "A variável query vai armazenar o texto que será usado para a pesquisa por similaridade. Note que ao criar o objeto docsearch a variável embeddings foi passada como parâmetro. Ela será usada para criar uma representação da query e comparar com os dados contidos no Pinecone. Até o momento, a Maritaca não tem essa opção de criar embeddings, então vamos usar a OpenAI mesmo."
      ],
      "metadata": {
        "id": "u7MeabBwFVy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the Sabiá Models?\" # faça uma pergunta ou escreva um texto.\n",
        "docs = docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "HQANoVRkFg4p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tegUBCIDGgdg",
        "outputId": "be44d3ce-4ef1-42e7-bebe-c17de063ac41"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='We extended the training of three models — LLaMA 7B and 65B [67] as well as GPT-J [70] — originally trained on English-centric corpora, on Portuguese texts; these further pretrained models from LLaMA are denoted as Sabi´a, while the one derived from GPT-J is referred to as Sabi´a-J.3\\n\\n3.2 Sabi´a models', metadata={}),\n",
              " Document(page_content='We evaluate the Sabi´a models on the Portuguese Evaluation Tasks (Poeta) benchmark, which comprises 14 downstream NLP datasets in Portuguese: ASSIN 2 RTE and STS [51], ENEM Challenge [60], ENEM 2022 [40], FaQuAD [56], TweetSentBr [5], AG News [79], IMDB [35], MASSIVE [15], MKQA [33], BoolQ [11], SST2 [61], WSC [13], and BLUEX [1]. Half of them (ASSIN 2 RTE and STS, BLUEX, ENEM Challenge, ENEM 2022, FaQuAD, and TweetSentBr) were originally written in Portuguese, and the remaining ones were either manually', metadata={}),\n",
              " Document(page_content='Sabi´a: Portuguese Large Language Models\\n\\nRamon Pires∗, Hugo Abonizio∗, Thales Sales Almeida∗, and Rodrigo Nogueira∗\\n\\nMaritaca AI\\n\\nAbstract\\n\\n3 2 0 2', metadata={}),\n",
              " Document(page_content='Starting from the LLaMA weights, we train the Sabi´a models on our Portuguese dataset (described in Section 3.1) using the t5x and seqio frameworks [52]. Adhering closely to the hyperparameters used by PALM, we use the AdaFactor optimizer [58] without factorization, a ﬁrst-order momentum β1 = 0.9, and a second-order momentum β2 = 1−k−0.8, where k represents the step number. We apply global norm clipping at 1.0 and dynamic weight decay of lr2, with lr denoting the current learning rate.', metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integração final\n",
        "\n",
        "Agora vamos usar os textos vindos do Pinecone e pedir para nosso LLM, o MariTalk, criar uma resposta adequada."
      ],
      "metadata": {
        "id": "PdOexA_rGtWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "H3J_9tSIlHxB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "Use os trechos a seguir para responder a pergunta no final. Se você não souber a resposta, apenas diga que não sabe, não tente criar uma resposta.\n",
        "\n",
        "{context}\n",
        "\n",
        "Pergunta: {question}\n",
        "Resposta:\n",
        "\"\"\"\n",
        "\n",
        "chain_prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=PROMPT)\n",
        "\n",
        "# note a variável llm sendo passada abaixo, ela é o Maritaca\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True, prompt=chain_prompt)"
      ],
      "metadata": {
        "id": "J5-b0BSXG6En"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the Sabiá Models?\" # faça uma pergunta ou escreva um texto.\n",
        "\n",
        "# no exemplo abaixo reduzimos o número de documentos para caber na janela do\n",
        "# Maritaca\n",
        "docs = docsearch.similarity_search(query, k=5)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "fGj09opwG-oB",
        "outputId": "73dfe765-9de5-4371-c06f-e0a03194c79d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Use os trechos a seguir para responder a pergunta no final. Se você não souber a resposta, apenas diga que não sabe, não tente criar uma resposta.\n",
            "\n",
            "We extended the training of three models — LLaMA 7B and 65B [67] as well as GPT-J [70] — originally trained on English-centric corpora, on Portuguese texts; these further pretrained models from LLaMA are denoted as Sabi´a, while the one derived from GPT-J is referred to as Sabi´a-J.3\n",
            "\n",
            "3.2 Sabi´a models\n",
            "\n",
            "We evaluate the Sabi´a models on the Portuguese Evaluation Tasks (Poeta) benchmark, which comprises 14 downstream NLP datasets in Portuguese: ASSIN 2 RTE and STS [51], ENEM Challenge [60], ENEM 2022 [40], FaQuAD [56], TweetSentBr [5], AG News [79], IMDB [35], MASSIVE [15], MKQA [33], BoolQ [11], SST2 [61], WSC [13], and BLUEX [1]. Half of them (ASSIN 2 RTE and STS, BLUEX, ENEM Challenge, ENEM 2022, FaQuAD, and TweetSentBr) were originally written in Portuguese, and the remaining ones were either manually\n",
            "\n",
            "Sabi´a: Portuguese Large Language Models\n",
            "\n",
            "Ramon Pires∗, Hugo Abonizio∗, Thales Sales Almeida∗, and Rodrigo Nogueira∗\n",
            "\n",
            "Maritaca AI\n",
            "\n",
            "Abstract\n",
            "\n",
            "3 2 0 2\n",
            "\n",
            "Starting from the LLaMA weights, we train the Sabi´a models on our Portuguese dataset (described in Section 3.1) using the t5x and seqio frameworks [52]. Adhering closely to the hyperparameters used by PALM, we use the AdaFactor optimizer [58] without factorization, a ﬁrst-order momentum β1 = 0.9, and a second-order momentum β2 = 1−k−0.8, where k represents the step number. We apply global norm clipping at 1.0 and dynamic weight decay of lr2, with lr denoting the current learning rate.\n",
            "\n",
            "for the smaller Sabi´a-J and Sabi´a-7B models. Notably, Sabi´a-65B marginally outperforms OpenAI’s GPT-3.5-turbo, which serves as the base model for ChatGPT.\n",
            "\n",
            "Pergunta: What are the Sabiá Models?\n",
            "Resposta:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Sabiá models refer to the Portuguese Large Language Models that were trained on Portuguese texts using the LLaMA and GPT-J models as a starting point.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As sacadas são movimentos simultâneos de ambos os olhos em duas ou mais fases para fixação na mesma direção. Elas podem ser usadas para estudar o comportamento ocular, os Movimentos Rápidos dos Olhos (REM) durante o estágio do sono, entre outras coisas."
      ],
      "metadata": {
        "id": "yDY3r4FhidNa"
      }
    }
  ]
}