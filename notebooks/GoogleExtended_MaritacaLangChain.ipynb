{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielblins/palestra_gdg/blob/main/notebooks/GoogleExtended_MaritacaLangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integração Maritaca - Langchain\n",
        "\n",
        "Temos um LLM para chamar de nosso! Os desenvolvedores da https://www.maritaca.ai estão de parabéns pelo excelente trabalho desenvolvido.\n",
        "\n",
        "O objetivo deste notebook é demonstrar como integrar o LLM ao Langchain considerando as chamadas a API que estão disponíveis no momento, que é a inferência apenas, não há possibilidade de obter os embeddings, mas acho que é uma questão de tempo até o time disponibilizar essa funcionalidade.\n",
        "\n",
        "Vamos ao que interessa."
      ],
      "metadata": {
        "id": "vhfhLRR_gdJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Software base\n",
        "Vamos instalar o langchain e algumas outras coisas"
      ],
      "metadata": {
        "id": "sDQljnkZhQun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xPT4kaRggZPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce666a9-7543-447e-979d-2148d699d455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q langchain unstructured pinecone-client openai tiktoken python-dotenv faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 1 - Criar um custom LLM\n",
        "\n",
        "Para mais informações sobre como o Langchain funciona, sugiro ler a documentação do produto. Aqui vamos focar na integração com o Maritaca.\n",
        "\n",
        "De todo o modo, o primeiro passo é criar uma extensão da classe LLM e assim entregar ao ambiente do langchain um objeto que \"sabe\" fazer chamadas ao Maritaca. Segundo a documentação, é preciso implementar o método _call que recebe uma string, executa a chamada ao LLM e retorna outra string como saída.\n",
        "\n",
        "Já que o nome do modelo é MariTalk, vamos usar esse nome para nossa classe especial."
      ],
      "metadata": {
        "id": "W2_EpgWkhyGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVFq1LiSZh02",
        "outputId": "fe22c8c0-c5fe-44b9-e6fa-0d0e0157ff74"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "import requests\n",
        "\n",
        "class MaritalkLLM(LLM):\n",
        "\n",
        "  @property\n",
        "  def _llm_type(self) -> str:\n",
        "    return \"custom\"\n",
        "\n",
        "  def _call(\n",
        "    self,\n",
        "    prompt: str,\n",
        "    stop: Optional[List[str]] = None,\n",
        "    run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "  ) -> str:\n",
        "    session = self.retry_request_session()\n",
        "    payload = {\n",
        "        \"messages\": [\n",
        "          {\n",
        "            \"role\": \"human\",\n",
        "            \"content\": prompt\n",
        "          }\n",
        "      ],\n",
        "      \"model\": 'Maritalk',\n",
        "      \"do_sample\": True,\n",
        "      \"max_tokens\": 2048,\n",
        "      \"temperature\": 0.5,\n",
        "      \"top_p\": 0.95,\n",
        "      \"repetition_penalty\": 1,\n",
        "      \"use_chat_template\": True,\n",
        "      \"stopping_tokens\": ['string']\n",
        "    }\n",
        "    res = session.post('https://chat.maritaca.ai/api/chat/inference',\n",
        "                       json=payload)\n",
        "\n",
        "    if res.status_code == 200:\n",
        "      return res.json()['answer']\n",
        "    else:\n",
        "      print(f'oops... status code {res.status_code}')\n",
        "      return ''\n",
        "\n",
        "  def retry_request_session(self, retries: Optional[int] = 3):\n",
        "    # we setup retry strategy to retry on common errors\n",
        "    retries = Retry(\n",
        "        total=retries,\n",
        "        backoff_factor=0.1,\n",
        "        status_forcelist=[\n",
        "            408,  # request timeout\n",
        "            429,  # too many requests\n",
        "            500,  # internal server error\n",
        "            502,  # bad gateway\n",
        "            503,  # service unavailable\n",
        "            504   # gateway timeout\n",
        "        ]\n",
        "    )\n",
        "    # we setup a session with the retry strategy\n",
        "    session = requests.Session()\n",
        "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "    return session\n",
        "\n",
        "  @property\n",
        "  def _identifying_params(self) -> Mapping[str, Any]:\n",
        "    \"\"\"Get the identifying parameters.\"\"\"\n",
        "    return {\"n\": 1}"
      ],
      "metadata": {
        "id": "CVHCT3pCiwjh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação\n",
        "\n",
        "A classe acima extende LLM da Langchain para que possamos implementar nossa lógica.\n",
        "\n",
        "O método retry_request_session é um helper que vai resolver pequenos problemas de comunicação que podem acontecer e retomar a chamada fazendo 3 tentativas.\n",
        "\n",
        "Em _call realizamos a chamada a API passando o prompt e os demais parâmetros.\n",
        "\n",
        "A seguir, o teste!"
      ],
      "metadata": {
        "id": "bOhBKUyur5xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = MaritalkLLM()"
      ],
      "metadata": {
        "id": "E3hyJvrismuy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm('Me explique como funciona o SUS')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "tDV-_1qxw6pt",
        "outputId": "b5873232-1ef8-412e-c2a8-ca353e0e3ab2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O SUS (Sistema Único de Saúde) é um sistema de saúde pública no Brasil que tem como objetivo garantir o acesso universal e igualitário à saúde para todos os brasileiros. Ele é financiado por recursos do governo federal, estadual e municipal, além de contribuições da população por meio de impostos.\\n\\nO SUS é composto por uma rede de serviços de saúde que inclui hospitais, postos de saúde, clínicas, laboratórios, entre outros. Ele é organizado em três níveis de atenção à saúde: a atenção básica, a atenção de média complexidade e a atenção de alta complexidade.\\n\\nA atenção básica é o primeiro contato da população com o sistema de saúde e é responsável por atender as necessidades de saúde mais comuns, como consultas médicas, exames preventivos e tratamento de doenças crônicas. Ela é realizada em postos de saúde e clínicas da família e é coordenada por equipes de saúde da família.\\n\\nA atenção de média complexidade é responsável por atender casos que exigem uma maior especialização e tecnologia, como exames de imagem, consultas com especialistas e cirurgias eletivas. Ela é realizada em hospitais e clínicas especializadas.\\n\\nA atenção de alta complexidade é responsável por atender casos que exigem alta tecnologia e recursos especializados, como transplantes, tratamento de câncer e traumas graves. Ela é realizada em hospitais de referência e centros de excelência em saúde.\\n\\nO SUS também tem programas de prevenção e controle de doenças, como o Programa Nacional de Imunizações, que oferece vacinas gratuitas para a população, e o Programa Nacional de DST/AIDS, que promove a prevenção e o tratamento da AIDS e outras doenças sexualmente transmissíveis.\\n\\nEm resumo, o SUS é um sistema de saúde público que tem como objetivo garantir o acesso universal e igualitário à saúde para todos os brasileiros, por meio de uma rede de serviços de saúde organizada em três níveis de atenção à saúde: a atenção básica, a atenção de média complexidade e a atenção de alta complexidade. Ele também tem programas de prevenção e controle de doenças.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O SUS (Sistema Único de Saúde) é um sistema de saúde pública no Brasil que tem como objetivo garantir o acesso universal e igualitário à saúde para todos os brasileiros. Ele é financiado por recursos do governo federal, estadual e municipal, além de contribuições da população por meio de impostos.\n",
        "\n",
        "O SUS é composto por uma rede de serviços de saúde que inclui hospitais, postos de saúde, clínicas, laboratórios, entre outros. Ele é organizado em três níveis de atenção à saúde: a atenção básica, a atenção de média complexidade e a atenção de alta complexidade.\n",
        "\n",
        "A atenção básica é o primeiro contato da população com o sistema de saúde e é responsável por atender as necessidades de saúde mais comuns, como consultas médicas, exames preventivos e tratamento de doenças crônicas. Ela é realizada em postos de saúde e clínicas da família e é coordenada por equipes de saúde da família.\n",
        "\n",
        "A atenção de média complexidade é responsável por atender casos que exigem uma maior especialização e tecnologia, como exames de imagem, consultas com especialistas e cirurgias eletivas. Ela é realizada em hospitais e clínicas especializadas.\n",
        "\n",
        "A atenção de alta complexidade é responsável por atender casos que exigem alta tecnologia e recursos especializados, como transplantes, tratamento de câncer e traumas graves. Ela é realizada em hospitais de referência e centros de excelência em saúde.\n",
        "\n",
        "O SUS também tem programas de prevenção e controle de doenças, como o Programa Nacional de Imunizações, que oferece vacinas gratuitas para a população, e o Programa Nacional de DST/AIDS, que promove a prevenção e o tratamento da AIDS e outras doenças sexualmente transmissíveis.\n",
        "\n",
        "Em resumo, o SUS é um sistema de saúde público que tem como objetivo garantir o acesso universal e igualitário à saúde para todos os brasileiros, por meio de uma rede de serviços de saúde organizada em três níveis de atenção à saúde: a atenção básica, a atenção de média complexidade e a atenção de alta complexidade. Ele também tem programas de prevenção e controle de doenças."
      ],
      "metadata": {
        "id": "R20dRP4zdlsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompts mais elaborados\n",
        "\n",
        "Agora que conseguimos integrar, vamos usar um prompt mais sofisticado e ver como o modelo se comporta. A idea a seguir é pedir ao modelo para entender um contexto, extrarir perguntas dele e nos entregar a resposta em um determinado padrão."
      ],
      "metadata": {
        "id": "L7CmNEXFy8ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "O trecho a seguir contem fatos sobre o assunto que ele trata. Sua missão é extrair dele 3 perguntas e organizar a resposta em formato JSON da seguinte maneira:\n",
        "[{\"pergunta1\": \"aqui vai a pergunta extraída\"},{\"pergunta2\": \"aqui vai a pergunta extraída\"}].\n",
        "\n",
        "Contexto:\n",
        "A revolta começou como uma série de protestos contra a cobrança de impostos e a falta de autonomia política para a província do Rio Grande do Sul.\n",
        "Em 1835, Bento Gonçalves liderou uma rebelião que resultou na proclamação da República Rio-Grandense, um governo independente que se opunha ao governo central brasileiro.\n",
        "\"\"\"\n",
        "\n",
        "llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HN0Z-sBGzQYA",
        "outputId": "a2cfeb68-aea4-4524-f0bd-d268aa7f3b85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[\\n{\"pergunta1\": \"Quando começou a Revolução Farroupilha?\"},\\n{\"pergunta2\": \"Quem liderou a Revolução Farroupilha?\"},\\n{\"pergunta3\": \"O que a Revolução Farroupilha buscava?\"}\\n]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integração com Pinecone\n",
        "\n",
        "Para realizar uma integração completa usando exclusivamente o Maritaca, a API precisaria de um endpoint para captura dos embeddings, mas isso ainda não está disponível, então, a seguir iremos utilizar a OpenAI para criar as representações dos textos, enviar para o Pinecone e usar o Maritaca no final do fluxo. Funciona assim:\n",
        "\n",
        "1. Capturamos os textos que queremos trabalhar, neste caso, a Portaria de Consolidação N 1 do Ministério da Saúde.\n",
        "\n",
        "2. Vamos quebrar o texto em pequenas partes.\n",
        "\n",
        "3. Interagir com a OpenAI e capturar os embeddings dos trechos criados.\n",
        "\n",
        "4. Enviar esses trechos para o Pinecone.\n",
        "\n",
        "Perceba que os passos acima são executados uma só vez ou sempre que houver necessidade de atualizar a estrutura. O resultado é uma base com diversos vetores que representam cada um dos trechos que criamos. O Pinecone, ou qualquer banco do mesmo tipo, viabiliza que sejam executadas consultas por similaridade. Dessa forma:\n",
        "\n",
        "1. Agora que temos a base podemos enviar uma pergunta ou um texto qualquer, mas temos que criar a representação da pergunta e neste momento iremos chamar novamente a OpenAI para captura dos embeddings.\n",
        "\n",
        "2. Em seguida, a Langchain interage com o Pinecone e pede para retornar os N vetores que mais se parecem com a pergunta/texto que enviamos.\n",
        "\n",
        "Deste ponto em diante, já temos acesso aos N trechos semelhantes e, portanto, são os que possívelmente respondem a nossa pergunta. Vamos usar eles como contexto do prompt final com Maritaca."
      ],
      "metadata": {
        "id": "2CKaxw0KzoiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import OnlinePDFLoader\n",
        "\n",
        "loader = OnlinePDFLoader(\"https://repositorio.ufrn.br/bitstream/123456789/46616/1/AvaliaçãoAssociaçãoEletrooculográficos_Oliveira_2022.pdf\")"
      ],
      "metadata": {
        "id": "dvQPhEoWznwY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "spnh8rpw_SYL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'Você tem {len(data)} documento(s) na base')\n",
        "print (f'Há {len(data[0].page_content)} caracteres no documento')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wBNh_VG_OD7",
        "outputId": "7c872efd-3dfb-4ac0-cfc8-70967198243d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Você tem 1 documento(s) na base\n",
            "Há 58236 caracteres no documento\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100, separators=['\\n', ' ', ''])\n",
        "texts = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "ceoaIfvi_2hi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'Agora há {len(texts)} documentos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FYQmtVtAAVM",
        "outputId": "06ee9b3e-b2f5-4b24-ea81-79955294adc7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agora há 152 documentos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[50].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qLVU_xlWAGMx",
        "outputId": "1f3de18f-9fef-44a2-d281-7a76442e4c4f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.3.1 Sacadas e microssacadas\\n\\nAs sacadas e microssacadas são tipos de movimentos oculares. As sacadas são\\n\\nmovimentos simultâneos de ambos os olhos em duas ou mais fases para fixação na mesma\\n\\ndireção (CASSIN; SOLOMON, 1990). As sacadas podem ser usadas para estudar o\\n\\ncomportamento ocular, os Movimentos Rápidos dos Olhos, tradicionalmente denominados\\n\\npela sigla em inglês de Rapid Eye Movements (REM) durante o estágio do sono, além de'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuração da OpenAI e Pinecone\n",
        "\n",
        "Recomendo ler como o Pinecone funciona, há muita documentação pela internet.\n",
        "\n",
        "O código a seguir realiza a integração entre Pinecone e OpenAI e para isso você vai precisar de chaves. Para manter as que uso de forma privada, criei um arquivo .env com o seguinte formato:\n",
        "\n",
        "```\n",
        "    OPENAI_API_KEY=key\n",
        "    PINECONE_API_KEY=key\n",
        "    PINECONE_API_ENV=key\n",
        "    PINECONE_INDEX=key\n",
        "```\n",
        "\n",
        "Chamei de .env e coloquei na raiz do colab. Você pode fazer o mesmo para testar este notebook."
      ],
      "metadata": {
        "id": "5RvC7gq7CHgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import pinecone\n",
        "import os"
      ],
      "metadata": {
        "id": "yBo08jDEAcj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a8bf07-7195-4ac8-b8a4-a25454a47880"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
        "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
        "PINECONE_API_ENV = os.environ['PINECONE_API_ENV']\n",
        "INDEX_NAME = os.environ['PINECONE_INDEX']"
      ],
      "metadata": {
        "id": "kPmRWM9WAvwb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "LTqfAXzrdnr7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    environment=PINECONE_API_ENV\n",
        ")\n",
        "index_name = INDEX_NAME"
      ],
      "metadata": {
        "id": "EW6Q-oY6Dks0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use o código a seguir para criar a base\n",
        "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name, namespace=index_name)\n",
        "\n",
        "# Use o código a seguir se já tiver um índice criado\n",
        "# docsearch = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)"
      ],
      "metadata": {
        "id": "pgHNDerwEeW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c9b010-d757-4067-8124-024f7ceb2d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-gahvafdz6yYTOuv97HO8hW1p on tokens per min. Limit: 150000 / min. Current: 145675 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-gahvafdz6yYTOuv97HO8hW1p on tokens per min. Limit: 150000 / min. Current: 143845 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-gahvafdz6yYTOuv97HO8hW1p on tokens per min. Limit: 150000 / min. Current: 141557 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-gahvafdz6yYTOuv97HO8hW1p on tokens per min. Limit: 150000 / min. Current: 139693 / min. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuração da FAISS vector store"
      ],
      "metadata": {
        "id": "UI8NwLl-jD74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "import pickle\n",
        "\n",
        "def store_embeddings(docs, embeddings, sotre_name, path):\n",
        "\n",
        "    vectorStore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(vectorStore, f)\n",
        "\n",
        "def load_embeddings(sotre_name, path):\n",
        "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"rb\") as f:\n",
        "        VectorStore = pickle.load(f)\n",
        "    return VectorStore\n"
      ],
      "metadata": {
        "id": "HfDb_80Aap43"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = FAISS.from_texts([t.page_content for t in texts], embeddings)"
      ],
      "metadata": {
        "id": "xUGe9aLMcHYM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base construída\n",
        "\n",
        "Agora temos nossa base vetorizada! Nosso PDF contendo os dados da Portaria de Consolidação já está no Pinecone, vamos testar!\n",
        "\n",
        "A variável query vai armazenar o texto que será usado para a pesquisa por similaridade. Note que ao criar o objeto docsearch a variável embeddings foi passada como parâmetro. Ela será usada para criar uma representação da query e comparar com os dados contidos no Pinecone. Até o momento, a Maritaca não tem essa opção de criar embeddings, então vamos usar a OpenAI mesmo."
      ],
      "metadata": {
        "id": "u7MeabBwFVy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"O que é sacada?\" # faça uma pergunta ou escreva um texto.\n",
        "docs = docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "HQANoVRkFg4p"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tegUBCIDGgdg",
        "outputId": "b7d89516-b8f0-41a0-adfc-9ae1a27b5213"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='1.3.1 Sacadas e microssacadas\\n\\nAs sacadas e microssacadas são tipos de movimentos oculares. As sacadas são\\n\\nmovimentos simultâneos de ambos os olhos em duas ou mais fases para fixação na mesma\\n\\ndireção (CASSIN; SOLOMON, 1990). As sacadas podem ser usadas para estudar o\\n\\ncomportamento ocular, os Movimentos Rápidos dos Olhos, tradicionalmente denominados\\n\\npela sigla em inglês de Rapid Eye Movements (REM) durante o estágio do sono, além de', metadata={}),\n",
              " Document(page_content='consolidado no meio acadêmico, possuir diversas bibliotecas para análise, o laboratório já\\n\\n17\\n\\npossui equipamentos para esse tipo de análise, e, ainda, os dados que serão utilizados já foram\\n\\ncoletados usando EOG.\\n\\n1.3 Padrões oculares\\n\\nExistem alguns padrões oculares de interesse a serem analisados neste trabalho, entre\\n\\neles piscadas e sacadas, não serão analisados microssacadas e nem períodos de fixação.\\n\\n1.3.1 Sacadas e microssacadas', metadata={}),\n",
              " Document(page_content='Nesta seção serão apresentados os resultados obtidos e em seguida as discussões\\n\\nsobre eles.\\n\\nInicialmente, é interessante verificar a distribuição dos dados gerados pelo algoritmo\\n\\nEOGERT na detecção dos eventos oculares (piscadas e sacadas)(figura 6).', metadata={}),\n",
              " Document(page_content='de ataque (GSA) (figura 7 e 9A), ou seja, quanto maior o ganho de sacadas menor será o GSA\\n\\ne vice-versa. Ainda vale ressaltar que o score de ataque está ligado a diferença entre vezes que\\n\\no jogador matou e morreu dentro do jogo. Para essa situação, a nossa hipótese é que o\\n\\nindivíduo que tem um ganho maior de sacadas seja um jogador mais parado e que joga mais\\n\\nescondido, com isso acaba observando mais o ambiente, porém acaba sendo caçado de forma', metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integração final\n",
        "\n",
        "Agora vamos usar os textos vindos do Pinecone e pedir para nosso LLM, o MariTalk, criar uma resposta adequada."
      ],
      "metadata": {
        "id": "PdOexA_rGtWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "Use os trechos a seguir para responder a pergunta no final. Se você não souber a resposta, apenas diga que não sabe, não tente criar uma resposta.\n",
        "\n",
        "{context}\n",
        "\n",
        "Pergunta: {question}\n",
        "Resposta:\n",
        "\"\"\"\n",
        "\n",
        "chain_prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=PROMPT)\n",
        "\n",
        "# note a variável llm sendo passada abaixo, ela é o Maritaca\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True, prompt=chain_prompt)"
      ],
      "metadata": {
        "id": "J5-b0BSXG6En"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"O que são sacadas?\" # faça uma pergunta ou escreva um texto.\n",
        "\n",
        "# no exemplo abaixo reduzimos o número de documentos para caber na janela do\n",
        "# Maritaca\n",
        "docs = docsearch.similarity_search(query, k=5)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fGj09opwG-oB",
        "outputId": "f139fe18-d188-4f5c-9605-81b3d7c626e9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Use os trechos a seguir para responder a pergunta no final. Se você não souber a resposta, apenas diga que não sabe, não tente criar uma resposta.\n",
            "\n",
            "1.3.1 Sacadas e microssacadas\n",
            "\n",
            "As sacadas e microssacadas são tipos de movimentos oculares. As sacadas são\n",
            "\n",
            "movimentos simultâneos de ambos os olhos em duas ou mais fases para fixação na mesma\n",
            "\n",
            "direção (CASSIN; SOLOMON, 1990). As sacadas podem ser usadas para estudar o\n",
            "\n",
            "comportamento ocular, os Movimentos Rápidos dos Olhos, tradicionalmente denominados\n",
            "\n",
            "pela sigla em inglês de Rapid Eye Movements (REM) durante o estágio do sono, além de\n",
            "\n",
            "consolidado no meio acadêmico, possuir diversas bibliotecas para análise, o laboratório já\n",
            "\n",
            "17\n",
            "\n",
            "possui equipamentos para esse tipo de análise, e, ainda, os dados que serão utilizados já foram\n",
            "\n",
            "coletados usando EOG.\n",
            "\n",
            "1.3 Padrões oculares\n",
            "\n",
            "Existem alguns padrões oculares de interesse a serem analisados neste trabalho, entre\n",
            "\n",
            "eles piscadas e sacadas, não serão analisados microssacadas e nem períodos de fixação.\n",
            "\n",
            "1.3.1 Sacadas e microssacadas\n",
            "\n",
            "Nesta seção serão apresentados os resultados obtidos e em seguida as discussões\n",
            "\n",
            "sobre eles.\n",
            "\n",
            "Inicialmente, é interessante verificar a distribuição dos dados gerados pelo algoritmo\n",
            "\n",
            "EOGERT na detecção dos eventos oculares (piscadas e sacadas)(figura 6).\n",
            "\n",
            "de ataque (GSA) (figura 7 e 9A), ou seja, quanto maior o ganho de sacadas menor será o GSA\n",
            "\n",
            "e vice-versa. Ainda vale ressaltar que o score de ataque está ligado a diferença entre vezes que\n",
            "\n",
            "o jogador matou e morreu dentro do jogo. Para essa situação, a nossa hipótese é que o\n",
            "\n",
            "indivíduo que tem um ganho maior de sacadas seja um jogador mais parado e que joga mais\n",
            "\n",
            "escondido, com isso acaba observando mais o ambiente, porém acaba sendo caçado de forma\n",
            "\n",
            "37\n",
            "\n",
            "5 DISCUSSÕES\n",
            "\n",
            "Os resultados obtidos levaram a pensar sobre quais as possíveis causas das\n",
            "\n",
            "correlações entre as variáveis de jogo e as variáveis de eventos oculares. Para lidar com isso\n",
            "\n",
            "levantamos algumas possíveis hipóteses.\n",
            "\n",
            "5.1 Presa\n",
            "\n",
            "Para a presa, foi observada uma correlação negativa entre o GS e o ganho do score\n",
            "\n",
            "de ataque (GSA) (figura 7 e 9A), ou seja, quanto maior o ganho de sacadas menor será o GSA\n",
            "\n",
            "Pergunta: O que são sacadas?\n",
            "Resposta:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As sacadas são movimentos simultâneos de ambos os olhos em duas ou mais fases para fixação na mesma direção. Elas podem ser usadas para estudar o comportamento ocular, os Movimentos Rápidos dos Olhos (REM) durante o estágio do sono, entre outras coisas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As sacadas são movimentos simultâneos de ambos os olhos em duas ou mais fases para fixação na mesma direção. Elas podem ser usadas para estudar o comportamento ocular, os Movimentos Rápidos dos Olhos (REM) durante o estágio do sono, entre outras coisas."
      ],
      "metadata": {
        "id": "yDY3r4FhidNa"
      }
    }
  ]
}